{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This document is written, referring to the following tutorials about tf_agents: \n",
    "    * [tutorials](https://github.com/tensorflow/agents/tree/master/docs/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import episodic_replay_buffer\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "\n",
    "from tf_agents.specs.tensor_spec import BoundedTensorSpec, TensorSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define an environment class as an inheritance from PyEnvironment\n",
    "Instances from this class represent first-order delay(FOD) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    \n",
    "    Y(s) = K/(1+T*s) * U(s)\n",
    "    \n",
    "    T * dy(t)/dt = - y(t) + K * u(t), t > 0, \n",
    "    y(0) = y_init.\n",
    "    \n",
    "    y(t+1) = (1-1/T) * y(t) + K / T * u(t), t = 1,2, ...\n",
    "    y(0) = y_init.\n",
    "    \n",
    "    x(t+1) = (1-1/T) * x(t) + K / T * u(t), t = 1,2, ...\n",
    "    x(0) = x_init, \n",
    "    y(t) = x(t).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, T = 10, K = 1.0, discount = 0.9, maxIteration = 100):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='observation')\n",
    "\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        self.time = 0\n",
    "        self.T = T\n",
    "        self.K = K\n",
    "        self.discount = discount\n",
    "        self.maxIteration = maxIteration\n",
    "    \n",
    "    def getInitialState(self):\n",
    "        return 0.\n",
    "    \n",
    "    def getObservation(self):\n",
    "        return np.array((self._state,), np.float32) # (1,)\n",
    "    \n",
    "    def getReward(self):\n",
    "        sv = 1.0\n",
    "        err = sv - self.getObservation()[0] \n",
    "        return np.abs(err) # (,)\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.time = 0\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        \n",
    "        return ts.restart(self.getObservation())\n",
    "\n",
    "    def _step(self, action):\n",
    "        # action: (1,)\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        if self.time < self.maxIteration:\n",
    "            \n",
    "            self._state = (1-1/self.T) * self._state + self.K/self.T * action[0]\n",
    "            \n",
    "            self.time += 1\n",
    "            return ts.transition(self.getObservation(), reward = self.getReward(), discount = self.discount)\n",
    "        else:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(self.getObservation(), reward = self.getReward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInstanceOfEnvironment():\n",
    "    env = MyEnv()\n",
    "    return env, tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEnvironmentUnitTests():\n",
    "    \n",
    "    def aSimpleUnitTest():\n",
    "        env,_ = createInstanceOfEnvironment()\n",
    "        assert isinstance(env, py_environment.PyEnvironment)\n",
    "        utils.validate_py_environment(env, episodes=5)\n",
    "\n",
    "    def anotherSimpleUnitTest():\n",
    "        env, _= createInstanceOfEnvironment()\n",
    "        assert isinstance(env, py_environment.PyEnvironment)\n",
    "\n",
    "        u = np.array(np.random.randn(1), np.float32) # (,)\n",
    "\n",
    "        time_step = env.reset()    \n",
    "        rewardAvg = time_step.reward    \n",
    "        for _ in range(10):\n",
    "            time_step = env.step(u)\n",
    "            rewardAvg = (1-1/10) * rewardAvg + 1/10 * time_step.reward\n",
    "\n",
    "    aSimpleUnitTest()\n",
    "    anotherSimpleUnitTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runEnvironmentUnitTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Represent P-controllers by deterministic policy networks or stochastic ones\n",
    "\n",
    "MyActionNetDeterminisitc and MyActionNetDistiributional are implementations of P-controller with saturated/bounded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDeterministic(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        # output_tensor_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)\n",
    "\n",
    "        return _actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDistributional(network.Network):\n",
    "    \"\"\"\n",
    "    \n",
    "    An instance as stochastic policy represents a P-controller with a random value generator.\n",
    "    \n",
    "    >> create an instance of the network:\n",
    "    net = MyActionNetDistributional(input_tensor_spec, output_tensor_spec)    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        self._log_action_std = tf.Variable(tf.zeros(shape=())) # (,)\n",
    "        # output_tensor_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)        \n",
    "        _action_std = tf.ones_like(_actions) * tf.math.exp(self._log_action_std) # (1, nMv)\n",
    "\n",
    "        return tfp.distributions.MultivariateNormalDiag(_actions, _action_std), network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInstanceOfActorNetwork(input_tensor_spec, output_tensor_spec, isStochastic = True):\n",
    "    if isStochastic:\n",
    "        return MyActionNetDistributional(input_tensor_spec, output_tensor_spec)\n",
    "    else:\n",
    "        return MyActionNetDeterministic(input_tensor_spec, output_tensor_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPolicyUnitTests():\n",
    "    nPv = 1\n",
    "    nMv = 1\n",
    "    batch_size = 2**5\n",
    "\n",
    "    input_tensor_spec = tensor_spec.TensorSpec((nPv,)\n",
    "                                               , tf.float32)\n",
    "\n",
    "    action_spec = tensor_spec.BoundedTensorSpec((nMv,),\n",
    "                                                tf.float32,\n",
    "                                                minimum=-1,\n",
    "                                                maximum=1)\n",
    "\n",
    "    \n",
    "    for actor_net in (createInstanceOfActorNetwork(input_tensor_spec, action_spec, isStochastic=True)\n",
    "                            ,createInstanceOfActorNetwork(input_tensor_spec, action_spec, isStochastic=False)):\n",
    "\n",
    "        my_actor_policy = actor_policy.ActorPolicy(\n",
    "            time_step_spec = ts.time_step_spec(input_tensor_spec),\n",
    "            action_spec    = action_spec,\n",
    "            actor_network  = actor_net)\n",
    "\n",
    "        observations = tf.random.normal(shape=(batch_size, nPv))\n",
    "\n",
    "        time_step = ts.restart(observations, batch_size) # time_step.is_first = True\n",
    "\n",
    "        action_step = my_actor_policy.action(time_step) # action_step.action: (*, nMv)\n",
    "\n",
    "        distribution_step = my_actor_policy.distribution(time_step)\n",
    "\n",
    "        assert isinstance(distribution_step.action, tfp.distributions.Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runPolicyUnitTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Apply an algorithm of RL to design controllers for FOD systems\n",
    "\n",
    "See [this tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/6_reinforce_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implement a factory method of sac agents *TMP*\n",
    "see also [this tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCriticNetwork(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec = TensorSpec(shape=(), dtype=tf.float32)):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='CriticNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "\n",
    "    def call(self, inputs, step_type, network_state = None):\n",
    "        del step_type\n",
    "\n",
    "        _inputs = tf.stack(inputs, axis=0) # (nPv + nMv,)\n",
    "        \n",
    "        _q = self._layer(_inputs) # (,)\n",
    "\n",
    "        return _q, network_state\n",
    "\n",
    "def createMyCriticNetworkInstance(observation_spec, action_spec):\n",
    "    critic_net = MyCriticNetwork((observation_spec, action_spec))\n",
    "    return critic_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInstanceOfTfAgent(train_env):\n",
    "    \n",
    "    critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "    actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "    alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "    target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "    target_update_period = 1 # @param {type:\"number\"}\n",
    "    gamma = 0.99 # @param {type:\"number\"}\n",
    "    reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "    \n",
    "    observation_spec, action_spec, time_step_spec = train_env.observation_spec(), train_env.action_spec(), train_env.time_step_spec()\n",
    "    \n",
    "    actor_net = createInstanceOfActorNetwork(observation_spec, action_spec, isStochastic = True)\n",
    "    \n",
    "    train_step = train_utils.create_train_step()\n",
    "\n",
    "    tf_agent = sac_agent.SacAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network=actor_net,\n",
    "        critic_network=createMyCriticNetworkInstance(observation_spec, action_spec),\n",
    "        critic_network_2 = createMyCriticNetworkInstance(observation_spec, action_spec),\n",
    "        target_critic_network=createMyCriticNetworkInstance(observation_spec, action_spec),\n",
    "        target_critic_network_2=createMyCriticNetworkInstance(observation_spec, action_spec),\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=alpha_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        train_step_counter=train_step)\n",
    "\n",
    "    tf_agent.initialize()\n",
    "    \n",
    "    return tf_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_env = createInstanceOfEnvironment()\n",
    "tf_agent = createInstanceOfTfAgent(train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implement a process collecting trajectories **TMP**\n",
    "\n",
    "see [this tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/5_replay_buffers_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ", train_env = createInstanceOfEnvironment()\n",
    "tf_agent = createInstanceOfTfAgent(train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnInstanceOfReplayBuffer(data_spec, batch_size = 1, replay_buffer_capacity = 1000):\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec,\n",
    "        batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "    \n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this document](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectTrajectories(train_env, policy, replay_buffer, collect_steps_per_iteration = 1):\n",
    "    # Add an observer that adds to the replay buffer:\n",
    "    replay_observer = [replay_buffer.add_batch]\n",
    "    \n",
    "    train_env.reset()\n",
    "\n",
    "    collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "        train_env,\n",
    "        policy,\n",
    "        observers=replay_observer,\n",
    "        num_steps=collect_steps_per_iteration).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer= createAnInstanceOfReplayBuffer(data_spec = tf_agent.collect_data_spec, batch_size=train_env.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.clear()\n",
    "\n",
    "nStep = 2**3\n",
    "collectTrajectories(train_env, tf_agent.collect_policy, replay_buffer, collect_steps_per_iteration = nStep)\n",
    "iter(replay_buffer.as_dataset(sample_batch_size=1, num_steps=nStep)).__next__()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Implement a process training agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining(train_env, tf_agent, nStepSimulation, num_iterations):\n",
    "    \n",
    "    collect_policy = tf_agent.collect_policy\n",
    "    \n",
    "    replay_buffer = createAnInstanceOfReplayBuffer(data_spec = tf_agent.collect_data_spec)\n",
    "    replay_buffer.clear()\n",
    "    \n",
    "    train_env.reset()\n",
    "    \n",
    "    tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        collectTrajectories(train_env= train_env\n",
    "                            , policy = collect_policy\n",
    "                            , replay_buffer = replay_buffer\n",
    "                            , collect_steps_per_iteration = nStepSimulation)\n",
    "\n",
    "        iterator = iter(replay_buffer.as_dataset(sample_batch_size=2**5, num_steps = nStepSimulation))        \n",
    "        trajectories, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrainingUnitTests():\n",
    "    num_iterations = 3\n",
    "    nStepSimulation = 2\n",
    "    _, train_env = createInstanceOfEnvironment()\n",
    "    tf_agent = createInstanceOfTfAgent(train_env)\n",
    "    \n",
    "    runTraining(train_env, tf_agent, nStepSimulation, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTrainingUnitTests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
