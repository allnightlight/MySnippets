{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This note offers a snippet of Soft-Actor Critic implemented on tf_agents.\n",
    "    + Sac agents are trained to control environments representing the first-order delay system.\n",
    "    + The episodes of environments must stop by a certain iteration, which means that this task is featured as an episodic-MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.tf_agent import TFAgent\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.replay_buffers.replay_buffer import ReplayBuffer\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories.time_step import TimeStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    \n",
    "    Y(s) = K/(1+T*s) * U(s)\n",
    "    \n",
    "    T * dy(t)/dt = - y(t) + K * u(t), t > 0, \n",
    "    y(0) = y_init.\n",
    "    \n",
    "    y(t+1) = (1-1/T) * y(t) + K / T * u(t), t = 1,2, ...\n",
    "    y(0) = y_init.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, discount, T = 10, K = 1.0, maxIteration = None, sv = 0.5, dv = 0.1):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='observation')\n",
    "\n",
    "        assert maxIteration is None\n",
    "        \n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        self.time = 0\n",
    "        self.T = T\n",
    "        self.K = K\n",
    "        self.discount = discount\n",
    "        self.sv = sv\n",
    "        self.dv = dv\n",
    "        self.beta = np.sqrt(1 - (1-1/T)**2)\n",
    "    \n",
    "    def getInitialState(self):\n",
    "        return 0.0\n",
    "    \n",
    "    def getObservation(self):\n",
    "        return np.array((self._state,), np.float32) # (1,)\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.time = 0\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        \n",
    "        return ts.restart(self.getObservation())\n",
    "\n",
    "    def _step(self, action):\n",
    "        # action: (1,)\n",
    "        \n",
    "        #>> print(action, type(action))\n",
    "        #[-0.57796675] <class 'numpy.ndarray'>\n",
    "        \n",
    "        w = np.random.randn() # (,)\n",
    "        self._state = (1-1/self.T) * self._state + self.K/self.T * action[0] + self.dv * self.beta * w            \n",
    "        self.time += 1\n",
    "        \n",
    "        reward = -np.abs(self.sv-self._state)\n",
    "        \n",
    "        # >> rtn = ts.transition(self.getObservation(), reward = reward, discount = self.discount)\n",
    "        # >> print(rtn, type(rtn))\n",
    "        # TimeStep(\n",
    "        #     {'discount': array(0.999, dtype=float32),\n",
    "        #      'observation': array([-0.06060384], dtype=float32),\n",
    "        #      'reward': array(-0.56060386, dtype=float32),\n",
    "        #      'step_type': array(1)}) <class 'tf_agents.trajectories.time_step.TimeStep'>\n",
    "        return ts.transition(self.getObservation(), reward = reward, discount = self.discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, train_env: TFPyEnvironment, train_agent: TFAgent, replay_buffer: ReplayBuffer):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        \n",
    "        self.train_env = train_env\n",
    "        self.train_agent = train_agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        \n",
    "    def collectTrajectories(self):\n",
    "        \n",
    "        while True:\n",
    "            time_step = self.train_env.current_time_step()\n",
    "            # >> print(time_step, type(time_step))\n",
    "            # TimeStep(\n",
    "            # {'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
    "            #  'observation': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
    "            #  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
    "            #  'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>}) <class 'tf_agents.trajectories.time_step.TimeStep'>\n",
    "\n",
    "            action_step = self.train_agent.collect_policy.action(time_step)\n",
    "            # >> print(action_step, type(action_step))\n",
    "            # PolicyStep(action=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.49118155]], dtype=float32)>, state=(), info=()) <class 'tf_agents.trajectories.policy_step.PolicyStep'>\n",
    "            \n",
    "            # >> print(action_step.action, type(action_step.action))\n",
    "            # tf.Tensor([[-0.49118155]], shape=(1, 1), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
    "            \n",
    "            next_time_step = self.train_env.step(action_step.action)    \n",
    "            # >> print(next_time_step, type(next_time_step))\n",
    "            # TimeStep(\n",
    "            # {'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.999], dtype=float32)>,\n",
    "            #  'observation': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.06888025]], dtype=float32)>,\n",
    "            #  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.56888026], dtype=float32)>,\n",
    "            #  'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>}) <class 'tf_agents.trajectories.time_step.TimeStep'>\n",
    "\n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            # >> print(traj, type(traj))\n",
    "            # Trajectory(\n",
    "            # {'action': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.49118155]], dtype=float32)>,\n",
    "            #  'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.999], dtype=float32)>,\n",
    "            #  'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
    "            #  'observation': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
    "            #  'policy_info': (),\n",
    "            #  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.56888026], dtype=float32)>,\n",
    "            #  'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>}) <class 'tf_agents.trajectories.trajectory.Trajectory'>\n",
    "\n",
    "            self.replay_buffer.add_batch(traj)\n",
    "\n",
    "            yield None\n",
    "\n",
    "    def run(self, num_train_iteration = 2**7, sample_batch_size = 2**5, num_steps = 2):\n",
    "        \n",
    "        self.replay_buffer.clear()        \n",
    "        self.train_env.reset()\n",
    "        \n",
    "        for i, _ in enumerate(self.collectTrajectories()):\n",
    "\n",
    "            if i >= num_train_iteration:\n",
    "                break\n",
    "                \n",
    "            size_of_replay_buffer = i + 1\n",
    "            if size_of_replay_buffer >= num_steps:\n",
    "                iterator = iter(self.replay_buffer.as_dataset(sample_batch_size = sample_batch_size, num_steps = num_steps))\n",
    "                trajectories, _ = next(iterator)\n",
    "                self.train_agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEvaluator(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, eval_env: TFPyEnvironment, train_agent: TFAgent, maxIteration = 100):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        \n",
    "        self.eval_env = eval_env\n",
    "        self.train_agent = train_agent\n",
    "        self.maxIteration = maxIteration\n",
    "                \n",
    "    def runSimulation(self) -> TimeStep:\n",
    "        '''\n",
    "        \n",
    "        Here is an example of action_step:\n",
    "            \n",
    "            PolicyStep(action=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>, state=(), info=())\n",
    "        \n",
    "        Here is an example of time_step or time_step_prev:\n",
    "        \n",
    "            TimeStep(\n",
    "                {'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
    "                 'observation': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.03926363]], dtype=float32)>,\n",
    "                 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0392636], dtype=float32)>,\n",
    "                 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>})        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        time_step = self.eval_env.reset()\n",
    "        time_step_prev = time_step \n",
    "        for _ in range(self.maxIteration):\n",
    "            action_step = self.train_agent.policy.action(time_step)\n",
    "            time_step = self.eval_env.step(action_step.action)\n",
    "            yield time_step_prev, action_step, time_step\n",
    "            time_step_prev = time_step\n",
    "            \n",
    "    def evaluate(self):\n",
    "        \n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        for _, action_step, time_step in self.runSimulation():\n",
    "            observations.append(time_step.observation.numpy()) # (1, *)\n",
    "            rewards.append(time_step.reward.numpy()) # (1,)\n",
    "            actions.append(action_step.action.numpy()) # (1, *)            \n",
    "        \n",
    "        observations = np.concatenate(observations, axis=0) # (nSample, nObservation)\n",
    "        actions = np.concatenate(actions, axis=0) # (nSample, nAction)\n",
    "        rewards = np.concatenate(rewards, axis=0) # (nSample,)\n",
    "        \n",
    "        return observations, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factory(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "\n",
    "    def createEnv(self, discount = 0.999) -> TFPyEnvironment:\n",
    "        env = MyEnv(discount = discount)\n",
    "        return tf_py_environment.TFPyEnvironment(env)\n",
    "    \n",
    "    def createAgent(self, train_env: TFPyEnvironment, learning_rate = 3e-2) -> TFAgent:\n",
    "\n",
    "        from tf_agents.agents.ddpg import critic_network\n",
    "        from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "        from tf_agents.networks import actor_distribution_network\n",
    "        \n",
    "        critic_learning_rate = learning_rate # @param {type:\"number\"}\n",
    "        actor_learning_rate = learning_rate # @param {type:\"number\"}\n",
    "        alpha_learning_rate = learning_rate # @param {type:\"number\"}\n",
    "        target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "        target_update_period = 1 # @param {type:\"number\"}\n",
    "        gamma = 1.0 # @param {type:\"number\"}\n",
    "        reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "        \n",
    "        nH = 2**5\n",
    "        actor_fc_layer_params = (nH, nH)\n",
    "        critic_joint_fc_layer_params = (nH, nH)\n",
    "        \n",
    "        observation_spec, action_spec, time_step_spec = train_env.observation_spec(), train_env.action_spec(), train_env.time_step_spec()\n",
    "        \n",
    "        train_step = train_utils.create_train_step()\n",
    "\n",
    "        actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "              observation_spec,\n",
    "              action_spec,\n",
    "              fc_layer_params=actor_fc_layer_params,\n",
    "              continuous_projection_net=(\n",
    "                  tanh_normal_projection_network.TanhNormalProjectionNetwork))\n",
    "\n",
    "        critic_net = critic_network.CriticNetwork(\n",
    "                (observation_spec, action_spec),\n",
    "                observation_fc_layer_params=None,\n",
    "                action_fc_layer_params=None,\n",
    "                joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                last_kernel_initializer='glorot_uniform')\n",
    "\n",
    "        tf_agent = sac_agent.SacAgent(\n",
    "              time_step_spec,\n",
    "              action_spec,\n",
    "              actor_network=actor_net,\n",
    "              critic_network=critic_net,\n",
    "              actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                  learning_rate=actor_learning_rate),\n",
    "              critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                  learning_rate=critic_learning_rate),\n",
    "              alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "                  learning_rate=alpha_learning_rate),\n",
    "              target_update_tau=target_update_tau,\n",
    "              target_update_period=target_update_period,\n",
    "              td_errors_loss_fn=tf.math.squared_difference,\n",
    "              gamma=gamma,\n",
    "              reward_scale_factor=reward_scale_factor,\n",
    "              train_step_counter=train_step)\n",
    "        \n",
    "        tf_agent.initialize()\n",
    "        \n",
    "        return tf_agent    \n",
    "    \n",
    "    def createAnInstanceOfReplayBuffer(self, train_agent: TFAgent, replay_buffer_capacity = 2**16) -> ReplayBuffer:\n",
    "    \n",
    "        replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec = train_agent.collect_data_spec,\n",
    "            batch_size = 1,\n",
    "            max_length=replay_buffer_capacity)\n",
    "    \n",
    "        return replay_buffer\n",
    "    \n",
    "    \n",
    "    def createTrainer(self, learning_rate = 3e-4, discount = 0.999):\n",
    "        \n",
    "        train_env = self.createEnv(discount = discount)\n",
    "        train_agent = self.createAgent(train_env, learning_rate)\n",
    "        replay_buffer = self.createAnInstanceOfReplayBuffer(train_agent)\n",
    "        \n",
    "        return MyTrainer(train_env, train_agent, replay_buffer)\n",
    "    \n",
    "    \n",
    "    def createEvaluator(self, train_agent: TFAgent):\n",
    "\n",
    "        eval_env = self.createEnv()        \n",
    "        return MyEvaluator(eval_env, train_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "def createView(fig, observations, actions):\n",
    "\n",
    "    trange = [0, 100]\n",
    "    fig.clf()\n",
    "    ax1 = fig.add_subplot(2,1,1)\n",
    "    ax2 = fig.add_subplot(2,1,2)\n",
    "\n",
    "    ax1.plot(observations)\n",
    "    ax1.set_title('Observation')\n",
    "    ax1.set_xlim(trange)\n",
    "    ax1.axhline(0.5, linestyle=\"--\", color=\"gray\", label = \"target\")\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim([-1, 1])\n",
    "\n",
    "    ax2.plot(actions)\n",
    "    ax2.set_title('Action')\n",
    "    ax2.set_xlim(trange)\n",
    "    ax2.axhline(0.5, linestyle=\"--\", color=\"gray\", label = \"nominal\")\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim([-1, 1])\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = Factory()\n",
    "\n",
    "train_env = factory.createEnv()\n",
    "train_agent = factory.createAgent(train_env)\n",
    "replay_buffer = factory.createAnInstanceOfReplayBuffer(train_agent)\n",
    "\n",
    "trainer = MyTrainer(train_env, train_agent, replay_buffer)    \n",
    "evaluator = factory.createEvaluator(train_agent = trainer.train_agent)\n",
    "\n",
    "observation0, action0, _ = evaluator.evaluate()    \n",
    "trainer.run(num_train_iteration=2**8)\n",
    "observation1, action1, _ = evaluator.evaluate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial agents can't regulate the PV well like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "createView(fig, observation0, action0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Envrionments controlled by trained agents follow a target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "createView(fig, observation1, action1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
