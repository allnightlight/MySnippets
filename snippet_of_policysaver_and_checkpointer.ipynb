{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a snippet of `PolicySaver` and `CheckPointer`.\n",
    "\n",
    "It consists of the following commands:\n",
    "* to create an REINFORCEMENT agent\n",
    "* to save the policy of the agent by a `PolicySaver`\n",
    "\t* and load the saved policy\n",
    "* to save the agent by a `CheckPointer`\n",
    "\t* and load the agent\n",
    "\n",
    "Note that this snippet skips any code of training agents because it's not necessary for saving agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a REINFORCEMENT agent:\n",
    "\n",
    "The codes of this section come from a [tf_agents tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/6_reinforce_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.train.utils import train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\" # @param {type:\"string\"}\n",
    "fc_layer_params = (100,)\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAgent():\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        train_env.observation_spec(),\n",
    "        train_env.action_spec(),\n",
    "        fc_layer_params=fc_layer_params)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    train_step_counter = train_utils.create_train_step()\n",
    "\n",
    "    tf_agent = reinforce_agent.ReinforceAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        optimizer=optimizer,\n",
    "        normalize_returns=True,\n",
    "        train_step_counter=train_step_counter)\n",
    "    tf_agent.initialize()\n",
    "    return tf_agent, train_step_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A snippet of using `PolicySaver`\n",
    "Please, refer to a [tutorial](https://www.tensorflow.org/agents/api_docs/python/tf_agents/policies/PolicySaver) about `PolicySaver`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import PolicySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 save the policy of the agent by a PolicySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_policy_label = \"a_saved_policy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent, train_step_counter = createAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a policy saver from a given policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = tf_agent.policy\n",
    "policy_saver = PolicySaver(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On training process, save a trained policy, occasionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# run training process ...\n",
    "#\n",
    "\n",
    "# save trained policies\n",
    "policy_saver.save(saved_policy_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. load the saved policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_policy = tf.saved_model.load(saved_policy_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(trained_policy));\n",
    "# >> <class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(saved_policy_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A snippet of using `CheckPointer`\n",
    "Please, refer to a [tutorial](https://www.tensorflow.org/agents/api_docs/python/tf_agents/policies/PolicySaver) about `PolicySaver`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. save the parameters of an agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.utils.common import Checkpointer\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pointer_folder_path = \"saved_agents\"\n",
    "if os.path.exists(check_pointer_folder_path):\n",
    "    shutil.rmtree(check_pointer_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCheckPointer(tf_agent, train_step_counter):\n",
    "    check_pointer = Checkpointer(check_pointer_folder_path, max_to_keep = 1, agent = tf_agent, global_step = train_step_counter)\n",
    "    return check_pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent, train_step_counter = createAgent()\n",
    "check_pointer = createCheckPointer(tf_agent, train_step_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On training process, save a trained policy, occasionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# run training process ...\n",
    "#\n",
    "\n",
    "train_step_counter = train_step_counter + 32\n",
    "\n",
    "# save trained policies\n",
    "check_pointer.save(global_step=train_step_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_agent.trainable_variables[0][0,:3], train_step_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. load the saved parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the old agent with a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent, train_step_counter = createAgent()\n",
    "print(tf_agent.trainable_variables[0][0,:3], train_step_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above agent has different values of parameters from the saved agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pointer = createCheckPointer(tf_agent, train_step_counter)\n",
    "print(tf_agent.trainable_variables[0][0,:3], train_step_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved parameters have been loaded, then the agent instance has the same values of parameters with the saved agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(check_pointer_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
