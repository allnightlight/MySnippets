{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This note provides a snippet of using replay buffers of Tf-agents, \n",
    "    * See [tutorials](https://github.com/tensorflow/agents/blob/master/docs/tutorials/5_replay_buffers_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInstanceOfReplayBuffer(nPv = 3, nMv = 2, batch_size = 1, max_length=2**10):\n",
    "    data_spec =  (\n",
    "        tf.TensorSpec([nPv,], tf.float32, 'observation')\n",
    "        , tf.TensorSpec([nMv,], tf.float32, 'action')\n",
    "        , tf.TensorSpec([nPv,], tf.float32, 'next_observation')\n",
    "        , tf.TensorSpec([], tf.float32, 'reward')\n",
    "        )\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length)\n",
    "    \n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectData(nMv = 2, nPv = 3, batch_size = 1, nSample = 2**7):\n",
    "    for _ in range(nSample):\n",
    "        observationBatch = tf.random.normal([batch_size, nPv])\n",
    "        actionBatch = tf.random.normal([batch_size, nMv])\n",
    "        nextObservationBatch = tf.random.normal([batch_size, nPv])\n",
    "        rewardBatch = tf.random.normal([batch_size,])\n",
    "        yield (observationBatch, actionBatch, nextObservationBatch, rewardBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collecDataAidedByDynamicStepDriver():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an instance of replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = createInstanceOfReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add batches of items in the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.clear()\n",
    "for aBatch in collectData():\n",
    "    replay_buffer.add_batch(aBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read items from the buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch_size = 2**5\n",
    "num_steps = 1\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=sample_batch_size\n",
    "    , num_steps=num_steps)\n",
    "trajectories, _ = iter(dataset).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>for trj in trajectories:\n",
    "# >>    print(trj.shape)\n",
    "#\n",
    "# (32, 1, 3)\n",
    "# (32, 1, 2)\n",
    "# (32, 1, 3)\n",
    "# (32, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a trajectory by running a closed-loop simulation of an environment and a policy:\n",
    "\n",
    "See this tutorial: [Train a Deep Q Network with TF-Agents](https://tensorflow.google.cn/agents/tutorials/1_dqn_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(suite_gym.load('CartPole-v0'))\n",
    "policy = random_tf_policy.RandomTFPolicy(time_step_spec=env.time_step_spec(), \n",
    "                                         action_spec=env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "time_step = env.current_time_step()\n",
    "action_step = policy.action(time_step)\n",
    "next_time_step = env.step(action_step.action)\n",
    "traj = trajectory.from_transition(time_step, action_step, next_time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect trajectories by using a DynamicStepDriver instance:\n",
    "\n",
    "See this tutorial: [Train a Deep Q Network with TF-Agents](https://tensorflow.google.cn/agents/tutorials/1_dqn_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pair of environment and policy instances, which are converted by tensorflow wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(suite_gym.load('CartPole-v0'))\n",
    "policy = random_tf_policy.RandomTFPolicy(time_step_spec=env.time_step_spec(), \n",
    "                                         action_spec=env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of replay buffer wrapped by tf-format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = policy.collect_data_spec,\n",
    "    batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect trajectories from closed loop between the environment and the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env,\n",
    "    policy,\n",
    "    observers=[replay_buffer.add_batch, ],\n",
    "    num_steps=13)\n",
    "\n",
    "nTrajectory = 3\n",
    "for _ in range(nTrajectory):\n",
    "    driver.run(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call a trajectory from the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trj, _ in replay_buffer.as_dataset().__iter__():\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect trajectories by using a DynamicEpisodeDriver instance:\n",
    "\n",
    "See this document: [tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_episode_driver/DynamicEpisodeDriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "#from tf_agents.replay_buffers import episodic_replay_buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pair of environment and policy instances, which are converted by tensorflow wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(suite_gym.load('CartPole-v0'))\n",
    "policy = random_tf_policy.RandomTFPolicy(time_step_spec=env.time_step_spec(), \n",
    "                                         action_spec=env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of replay buffer wrapped by tf-format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = policy.collect_data_spec,\n",
    "    batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect trajectories from closed loop between the environment and the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    env,\n",
    "    policy,\n",
    "    observers=[replay_buffer.add_batch, ],\n",
    "    num_episodes=1)\n",
    "\n",
    "replay_buffer.clear()\n",
    "\n",
    "nTrajectory = 1\n",
    "for _ in range(nTrajectory):\n",
    "    driver.run(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call a trajectory from the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for trj, _ in itertools.islice(replay_buffer.as_dataset(sample_batch_size=1, num_steps=3).__iter__(),10):\n",
    "    print(trj.is_last())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample episodes from episodic replay buffer without using any driver\n",
    "\n",
    "+ [Using EpisodicReplayBuffer in TF-Agents](https://stackoverflow.com/questions/65397939/using-episodicreplaybuffer-in-tf-agents)\n",
    "+ [episodic_replay_buffer.py](https://github.com/tensorflow/agents/blob/master/tf_agents/replay_buffers/episodic_replay_buffer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import episodic_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pair of environment and policy instances, which are converted by tensorflow wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(suite_gym.load('CartPole-v0'))\n",
    "policy = random_tf_policy.RandomTFPolicy(time_step_spec=env.time_step_spec(), \n",
    "                                         action_spec=env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of replay buffer wrapped by tf-format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n",
    "    data_spec = policy.collect_data_spec,\n",
    "    capacity = 1000,\n",
    "    completed_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect trajectories from closed loop between the environment and the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.clear()\n",
    "\n",
    "collect_episodes_per_iteration = 3\n",
    "for _ in range(collect_episodes_per_iteration):    \n",
    "    \n",
    "    id_eps = tf.constant((-1,), dtype = tf.int64)\n",
    "    \n",
    "    env.reset()    \n",
    "    while True:\n",
    "        time_step = env.current_time_step()\n",
    "        if time_step.is_last():\n",
    "            break\n",
    "        else:\n",
    "            action_step = policy.action(time_step)\n",
    "            next_time_step = env.step(action_step.action)    \n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            id_eps = replay_buffer.add_batch(traj, id_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call some trajectories from the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trj, _ in replay_buffer.as_dataset(sample_batch_size=2**3, num_steps=2**4).__iter__():\n",
    "    print(\">>is first\")\n",
    "    print(trj.is_first())\n",
    "    print(\">>is last\")\n",
    "    print(trj.is_last())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trj, _ in replay_buffer.as_dataset().__iter__():\n",
    "    print(\">>is first\")\n",
    "    print(trj.is_first())\n",
    "    print(\">>is last\")\n",
    "    print(trj.is_last())\n",
    "    print(\">> discount\")\n",
    "    print(trj.discount)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
