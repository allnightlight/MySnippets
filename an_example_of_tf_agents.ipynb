{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This document is written, referring to the following tutorials about tf_agents: \n",
    "    * [tutorials](https://github.com/tensorflow/agents/tree/master/docs/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define an environment class as an inheritance from PyEnvironment\n",
    "Instances from this class represent first-order delay(FOD) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    \n",
    "    Y(s) = K/(1+T*s) * U(s)\n",
    "    \n",
    "    T * dy(t)/dt = - y(t) + K * u(t), t > 0, \n",
    "    y(0) = y_init.\n",
    "    \n",
    "    y(t+1) = (1-1/T) * y(t) + K / T * u(t), t = 1,2, ...\n",
    "    y(0) = y_init.\n",
    "    \n",
    "    x(t+1) = (1-1/T) * x(t) + K / T * u(t), t = 1,2, ...\n",
    "    x(0) = x_init, \n",
    "    y(t) = x(t).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, nStepSimulation = 100, T = 10, K = 1.0, discount = 0.9):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='observation')\n",
    "\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        self.time = 0\n",
    "        self.nStepSimulation = nStepSimulation\n",
    "        self.T = T\n",
    "        self.K = K\n",
    "        self.discount = discount\n",
    "    \n",
    "    def getInitialState(self):\n",
    "        return 0.\n",
    "    \n",
    "    def getObservation(self):\n",
    "        return np.array((self._state,), np.float32) # (1,)\n",
    "    \n",
    "    def getReward(self):\n",
    "        sv = 1.0\n",
    "        err = sv - self.getObservation()[0] \n",
    "        return np.abs(err) # (,)\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.time = 0\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        \n",
    "        return ts.restart(self.getObservation())\n",
    "\n",
    "    def _step(self, action):\n",
    "        # action: (1,)\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        if self.time < self.nStepSimulation:\n",
    "            \n",
    "            self._state = (1-1/self.T) * self._state + self.K/self.T * action[0]\n",
    "            \n",
    "            self.time += 1\n",
    "            return ts.transition(self.getObservation(), reward = self.getReward(), discount = self.discount)\n",
    "        else:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(self.getObservation(), reward = self.getReward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aSimpleUnitTest():\n",
    "    env = MyEnv()\n",
    "    assert isinstance(env, py_environment.PyEnvironment)\n",
    "    utils.validate_py_environment(env, episodes=5)\n",
    "\n",
    "def anotherSimpleUnitTest():\n",
    "    env = MyEnv()\n",
    "    assert isinstance(env, py_environment.PyEnvironment)\n",
    "\n",
    "    u = np.array(np.random.randn(1), np.float32) # (,)\n",
    "    \n",
    "    time_step = env.reset()    \n",
    "    rewardAvg = time_step.reward    \n",
    "    while not time_step.is_last():\n",
    "        time_step = env.step(u)\n",
    "        rewardAvg = (1-1/10) * rewardAvg + 1/10 * time_step.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSimpleUnitTest()\n",
    "anotherSimpleUnitTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Represent P-controllers by deterministic policy networks or stochastic ones\n",
    "\n",
    "MyActionNetDeterminisitc and MyActionNetDistiributional are implementations of P-controller with saturated/bounded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.policies import actor_policy\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDeterministic(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(action_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        # action_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)\n",
    "\n",
    "        return _actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDistributional(network.Network):\n",
    "    \"\"\"\n",
    "    \n",
    "    An instance as stochastic policy represents a P-controller with a random value generator.\n",
    "    \n",
    "    >> create an instance of the network:\n",
    "    net = MyActionNetDistributional(input_tensor_spec, output_tensor_spec)    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(action_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        self._log_action_std = tf.Variable(tf.zeros(shape=())) # (,)\n",
    "        # action_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)        \n",
    "        _action_std = tf.ones_like(_actions) * tf.math.exp(self._log_action_std) # (1, nMv)\n",
    "\n",
    "        return tfp.distributions.MultivariateNormalDiag(_actions, _action_std), network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnInstanceOfDeterministicPolicy(input_tensor_spec, action_spec):\n",
    "    time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "    return actor_policy.ActorPolicy(\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec    = action_spec,\n",
    "        actor_network  = MyActionNetDeterministic(input_tensor_spec, action_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnInstanceOfDistributionalPolicy(input_tensor_spec, action_spec):\n",
    "    time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "    return actor_policy.ActorPolicy(\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec    = action_spec,\n",
    "        actor_network  = MyActionNetDistributional(input_tensor_spec, action_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPv = 1\n",
    "nMv = 1\n",
    "batch_size = 2**5\n",
    "\n",
    "input_tensor_spec = tensor_spec.TensorSpec((nPv,)\n",
    "                                           , tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec((nMv,),\n",
    "                                            tf.float32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "\n",
    "for my_actor_policy in (createAnInstanceOfDeterministicPolicy(input_tensor_spec, action_spec)\n",
    "                        ,createAnInstanceOfDistributionalPolicy(input_tensor_spec, action_spec)):\n",
    "\n",
    "    observations = tf.random.normal(shape=(batch_size, nPv))\n",
    "\n",
    "    time_step = ts.restart(observations, batch_size) # time_step.is_first = True\n",
    "\n",
    "    action_step = my_actor_policy.action(time_step) # action_step.action: (*, nMv)\n",
    "\n",
    "    distribution_step = my_actor_policy.distribution(time_step)\n",
    "    \n",
    "    assert isinstance(distribution_step.action, tfp.distributions.Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement data collectors aided by replay buffers\n",
    "\n",
    "See [this tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/5_replay_buffers_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMv = 1\n",
    "nPv = 1\n",
    "batch_size = 2**5\n",
    "sample_batch_size = 2**2\n",
    "max_length = 2**10\n",
    "num_steps = 4 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an instance of replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec =  (\n",
    "    tf.TensorSpec([nMv,], tf.float32, 'action'),\n",
    "    tf.TensorSpec([nPv,], tf.float32, 'observation'),\n",
    "    tf.TensorSpec([], tf.float32, 'reward'),\n",
    "    )\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add batches of items in the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectData():\n",
    "    \"\"\"\n",
    "    Everytime a batch of items is collected, stream it out\n",
    "    \"\"\"\n",
    "    for _ in range(10):\n",
    "        actionBatch = tf.random.normal([batch_size, nMv])\n",
    "        observationBatch = tf.random.normal([batch_size, nPv])\n",
    "        rewardBatch = tf.random.normal([batch_size,])\n",
    "        yield (actionBatch, observationBatch, rewardBatch)\n",
    "        \n",
    "replay_buffer.clear()\n",
    "for (actionBatch, observationBatch, rewardBatch) in collectData():\n",
    "    replay_buffer.add_batch((actionBatch, observationBatch, rewardBatch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read items from the buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=sample_batch_size\n",
    "    , num_steps=num_steps)\n",
    "trajectories, _ = iter(dataset).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first item of trajectories represents a trajectory of actions, with shape(batch_size, num_steps, nMv)=\", trajectories[0].shape)\n",
    "print(\", the second, observations, with shape(batch_size, num_steps, nPv)=\", trajectories[1].shape)\n",
    "print(\"and the last, rewards, with shape(batch_size, num_steps)=\", trajectories[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Apply an algorithm of RL to design controllers for FOD systems\n",
    "\n",
    "See [this tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/6_reinforce_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 250 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 2 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
    "\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "log_interval = 25 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 create environment instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnEnvironmentInstance():\n",
    "    return MyEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an instance of MyEnvironment and check `time_step_spec` and `action_spec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = createAnEnvironmentInstance()\n",
    "env.reset()\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cast python-form environments to TFPyEnvironment and check if all the specifications of variables are wrapped by `TensorSpec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = createAnEnvironmentInstance()\n",
    "eval_py_env = createAnEnvironmentInstance()\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 create an agent instance and generate the data-collect policy and the evaluation one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createActionNetworkInstance(env):\n",
    "    return MyActionNetDistributional(input_tensor_spec = env.observation_spec()\n",
    "                                     , output_tensor_spec = env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an instance of stochastic action network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = createActionNetworkInstance(train_env)\n",
    "print(\"Input spec.:\")\n",
    "print(actor_net._input_tensor_spec)\n",
    "print(\"Output spec.:\")\n",
    "print(actor_net._output_tensor_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create two policies, the one policy to be deployed and the other to be used for collecting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 create an instance of ReplayBuffer and define a process to collect trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnInstanceOfReplayBuffer(data_spec, batch_size=1, max_length=2**10):\n",
    "    return tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec = data_spec\n",
    "        , batch_size=batch_size\n",
    "        , max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectTrajectories(environment, policy, replay_buffer, policy_state = ()):\n",
    "    replay_buffer.clear()\n",
    "    time_step = environment.reset()    \n",
    "    for _ in range(collect_episodes_per_iteration):    \n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        next_time_step = environment.step(action_step)\n",
    "        traj = trajectory.Trajectory(\n",
    "            time_step.step_type,\n",
    "            time_step.observation,\n",
    "            action_step.action,\n",
    "            action_step.info,\n",
    "            next_time_step.step_type,\n",
    "            next_time_step.reward,\n",
    "            next_time_step.discount)\n",
    "\n",
    "        replay_buffer.add_batch(traj)\n",
    "\n",
    "        time_step = next_time_step\n",
    "        policy_state = action_step.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = createAnInstanceOfReplayBuffer(data_spec = tf_agent.collect_data_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do a test-run of data collect steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectTrajectories(environment = train_env\n",
    "                    , policy = collect_policy\n",
    "                    , replay_buffer =  replay_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
