{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This document is written, referring to the following tutorials about tf_agents: \n",
    "    * [tutorials](https://github.com/tensorflow/agents/tree/master/docs/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import episodic_replay_buffer\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define an environment class as an inheritance from PyEnvironment\n",
    "Instances from this class represent first-order delay(FOD) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    \n",
    "    Y(s) = K/(1+T*s) * U(s)\n",
    "    \n",
    "    T * dy(t)/dt = - y(t) + K * u(t), t > 0, \n",
    "    y(0) = y_init.\n",
    "    \n",
    "    y(t+1) = (1-1/T) * y(t) + K / T * u(t), t = 1,2, ...\n",
    "    y(0) = y_init.\n",
    "    \n",
    "    x(t+1) = (1-1/T) * x(t) + K / T * u(t), t = 1,2, ...\n",
    "    x(0) = x_init, \n",
    "    y(t) = x(t).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, nStepSimulation = 100, T = 10, K = 1.0, discount = 0.9):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=(-1,), maximum=(1,), name='observation')\n",
    "\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        self.time = 0\n",
    "        self.nStepSimulation = nStepSimulation\n",
    "        self.T = T\n",
    "        self.K = K\n",
    "        self.discount = discount\n",
    "    \n",
    "    def getInitialState(self):\n",
    "        return 0.\n",
    "    \n",
    "    def getObservation(self):\n",
    "        return np.array((self._state,), np.float32) # (1,)\n",
    "    \n",
    "    def getReward(self):\n",
    "        sv = 1.0\n",
    "        err = sv - self.getObservation()[0] \n",
    "        return np.abs(err) # (,)\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.time = 0\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        \n",
    "        return ts.restart(self.getObservation())\n",
    "\n",
    "    def _step(self, action):\n",
    "        # action: (1,)\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        if self.time < self.nStepSimulation:\n",
    "            \n",
    "            self._state = (1-1/self.T) * self._state + self.K/self.T * action[0]\n",
    "            \n",
    "            self.time += 1\n",
    "            return ts.transition(self.getObservation(), reward = self.getReward(), discount = self.discount)\n",
    "        else:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(self.getObservation(), reward = self.getReward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInstanceOfEnvironment(nStepSimulation = 100):\n",
    "    return MyEnv(nStepSimulation = nStepSimulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEnvironmentUnitTests():\n",
    "    \n",
    "    def aSimpleUnitTest():\n",
    "        env = createInstanceOfEnvironment()\n",
    "        assert isinstance(env, py_environment.PyEnvironment)\n",
    "        utils.validate_py_environment(env, episodes=5)\n",
    "\n",
    "    def anotherSimpleUnitTest():\n",
    "        env = createInstanceOfEnvironment()\n",
    "        assert isinstance(env, py_environment.PyEnvironment)\n",
    "\n",
    "        u = np.array(np.random.randn(1), np.float32) # (,)\n",
    "\n",
    "        time_step = env.reset()    \n",
    "        rewardAvg = time_step.reward    \n",
    "        while not time_step.is_last():\n",
    "            time_step = env.step(u)\n",
    "            rewardAvg = (1-1/10) * rewardAvg + 1/10 * time_step.reward\n",
    "\n",
    "    aSimpleUnitTest()\n",
    "    anotherSimpleUnitTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runEnvironmentUnitTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Represent P-controllers by deterministic policy networks or stochastic ones\n",
    "\n",
    "MyActionNetDeterminisitc and MyActionNetDistiributional are implementations of P-controller with saturated/bounded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDeterministic(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        # output_tensor_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)\n",
    "\n",
    "        return _actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDistributional(network.Network):\n",
    "    \"\"\"\n",
    "    \n",
    "    An instance as stochastic policy represents a P-controller with a random value generator.\n",
    "    \n",
    "    >> create an instance of the network:\n",
    "    net = MyActionNetDistributional(input_tensor_spec, output_tensor_spec)    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(output_tensor_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        self._log_action_std = tf.Variable(tf.zeros(shape=())) # (,)\n",
    "        # output_tensor_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)        \n",
    "        _action_std = tf.ones_like(_actions) * tf.math.exp(self._log_action_std) # (1, nMv)\n",
    "\n",
    "        return tfp.distributions.MultivariateNormalDiag(_actions, _action_std), network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInstanceOfActorNetwork(input_tensor_spec, output_tensor_spec, isStochastic = True):\n",
    "    if isStochastic:\n",
    "        return MyActionNetDistributional(input_tensor_spec, output_tensor_spec)\n",
    "    else:\n",
    "        return MyActionNetDeterministic(input_tensor_spec, output_tensor_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPolicyUnitTests():\n",
    "    nPv = 1\n",
    "    nMv = 1\n",
    "    batch_size = 2**5\n",
    "\n",
    "    input_tensor_spec = tensor_spec.TensorSpec((nPv,)\n",
    "                                               , tf.float32)\n",
    "\n",
    "    action_spec = tensor_spec.BoundedTensorSpec((nMv,),\n",
    "                                                tf.float32,\n",
    "                                                minimum=-1,\n",
    "                                                maximum=1)\n",
    "\n",
    "    \n",
    "    for actor_net in (createInstanceOfActorNetwork(input_tensor_spec, action_spec, isStochastic=True)\n",
    "                            ,createInstanceOfActorNetwork(input_tensor_spec, action_spec, isStochastic=False)):\n",
    "\n",
    "        my_actor_policy = actor_policy.ActorPolicy(\n",
    "            time_step_spec = ts.time_step_spec(input_tensor_spec),\n",
    "            action_spec    = action_spec,\n",
    "            actor_network  = actor_net)\n",
    "\n",
    "        observations = tf.random.normal(shape=(batch_size, nPv))\n",
    "\n",
    "        time_step = ts.restart(observations, batch_size) # time_step.is_first = True\n",
    "\n",
    "        action_step = my_actor_policy.action(time_step) # action_step.action: (*, nMv)\n",
    "\n",
    "        distribution_step = my_actor_policy.distribution(time_step)\n",
    "\n",
    "        assert isinstance(distribution_step.action, tfp.distributions.Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runPolicyUnitTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Apply an algorithm of RL to design controllers for FOD systems\n",
    "\n",
    "See [this tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/6_reinforce_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implement a factory method of REINFORCE agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInstanceOfTfAgent(train_env, learning_rate = 1e-3):\n",
    "    \n",
    "    actor_network = createInstanceOfActorNetwork(input_tensor_spec = train_env.observation_spec()\n",
    "                                                           , output_tensor_spec = train_env.action_spec())\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "    tf_agent = reinforce_agent.ReinforceAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_network,\n",
    "        optimizer=optimizer,\n",
    "        normalize_returns=True,\n",
    "        train_step_counter=train_step_counter)\n",
    "    tf_agent.initialize()\n",
    "    \n",
    "    return tf_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTfAgentUnitTests():\n",
    "    train_py_env = createInstanceOfEnvironment()\n",
    "    train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "    createInstanceOfTfAgent(train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTfAgentUnitTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implement a process collecting trajectories\n",
    "\n",
    "+ [Using EpisodicReplayBuffer in TF-Agents](https://stackoverflow.com/questions/65397939/using-episodicreplaybuffer-in-tf-agents)\n",
    "+ [episodic_replay_buffer.py](https://github.com/tensorflow/agents/blob/master/tf_agents/replay_buffers/episodic_replay_buffer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnInstanceOfReplayBuffer(data_spec, max_length=2**10):\n",
    "    return episodic_replay_buffer.EpisodicReplayBuffer(data_spec\n",
    "                                                   , completed_only = True\n",
    "                                                   , capacity=max_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectTrajectories(environment, policy, replay_buffer, policy_state = (), collect_episodes_per_iteration = 1):\n",
    "    replay_buffer.clear()    \n",
    "    for iEps in range(collect_episodes_per_iteration):    \n",
    "        time_step = environment.reset()    \n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step, policy_state)\n",
    "            next_time_step = environment.step(action_step)\n",
    "            traj = trajectory.Trajectory(\n",
    "                time_step.step_type,\n",
    "                time_step.observation,\n",
    "                action_step.action,\n",
    "                action_step.info,\n",
    "                next_time_step.step_type,\n",
    "                next_time_step.reward,\n",
    "                next_time_step.discount)\n",
    "\n",
    "            replay_buffer.add_batch(traj, tf.constant((iEps,), dtype=tf.int64))\n",
    "\n",
    "            time_step = next_time_step\n",
    "            policy_state = action_step.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runReplayBufferUnitTests():\n",
    "    \n",
    "    nStepSimulation = 3\n",
    "\n",
    "    for _ in range(10):\n",
    "        \n",
    "        train_env = tf_py_environment.TFPyEnvironment(createInstanceOfEnvironment(nStepSimulation = nStepSimulation))\n",
    "\n",
    "        tf_agent = createInstanceOfTfAgent(train_env)\n",
    "\n",
    "        replay_buffer = createAnInstanceOfReplayBuffer(data_spec = tf_agent.collect_data_spec)\n",
    "\n",
    "        collect_policy = tf_agent.collect_policy\n",
    "    \n",
    "        collectTrajectories(environment = train_env\n",
    "                            , policy = collect_policy\n",
    "                            , replay_buffer =  replay_buffer)\n",
    "\n",
    "        iterator = iter(replay_buffer.as_dataset(sample_batch_size=1, num_steps = nStepSimulation+1))        \n",
    "        trajectories, _ = next(iterator)\n",
    "\n",
    "        assert trajectories.is_last().numpy()[0, -1], trajectories.is_last().numpy()\n",
    "        assert np.all(~trajectories.is_last().numpy()[0, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runReplayBufferUnitTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Implement a process training agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining(train_env, tf_agent, nStepSimulation, num_iterations):\n",
    "    \n",
    "    collect_policy = tf_agent.collect_policy\n",
    "    \n",
    "    replay_buffer = createAnInstanceOfReplayBuffer(data_spec = tf_agent.collect_data_spec)\n",
    "    \n",
    "    tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        collectTrajectories(environment = train_env\n",
    "                            , policy = collect_policy\n",
    "                            , replay_buffer = replay_buffer)\n",
    "\n",
    "        #XXX: num_steps must be nStepSimulation + 1 since an episode has nStepSimulation-trajectories and a terminated one.\n",
    "        iterator = iter(replay_buffer.as_dataset(sample_batch_size=1, num_steps = nStepSimulation + 1))        \n",
    "        trajectories, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience=trajectories)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrainingUnitTests():\n",
    "    num_iterations = 2\n",
    "    nStepSimulation = 3\n",
    "    train_env = tf_py_environment.TFPyEnvironment(createInstanceOfEnvironment(nStepSimulation = nStepSimulation))\n",
    "    tf_agent = createInstanceOfTfAgent(train_env)\n",
    "\n",
    "    runTraining(train_env, tf_agent, nStepSimulation, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTrainingUnitTests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
