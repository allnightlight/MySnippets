{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This document is written, referring to the following tutorials about tf_agents: \n",
    "    * [tutorials](https://github.com/tensorflow/agents/tree/master/docs/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define an environment class as an inheritance from PyEnvironment\n",
    "Instances from this class represent first-order delay systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    \n",
    "    Y(s) = K/(1+T*s) * U(s)\n",
    "    \n",
    "    T * dy(t)/dt = - y(t) + K * u(t), t > 0, \n",
    "    y(0) = y_init.\n",
    "    \n",
    "    y(t+1) = (1-1/T) * y(t) + K / T * u(t), t = 1,2, ...\n",
    "    y(0) = y_init.\n",
    "    \n",
    "    x(t+1) = (1-1/T) * x(t) + K / T * u(t), t = 1,2, ...\n",
    "    x(0) = x_init, \n",
    "    y(t) = x(t).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, nStepSimulation = 100, T = 10, K = 1.0, discount = 0.9):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.float32, minimum=-1, maximum=1, name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.float32, minimum=-1, maximum=1, name='observation')\n",
    "\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        self.time = 0\n",
    "        self.nStepSimulation = nStepSimulation\n",
    "        self.T = T\n",
    "        self.K = K\n",
    "        self.discount = discount\n",
    "    \n",
    "    def getInitialState(self):\n",
    "        return 0.\n",
    "    \n",
    "    def getObservation(self):\n",
    "        return np.array(self._state, np.float32) # (,)\n",
    "    \n",
    "    def getReward(self):\n",
    "        sv = 1.0\n",
    "        err = sv - self.getObservation()\n",
    "        return np.abs(err)\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.time = 0\n",
    "        self._state = self.getInitialState()\n",
    "        self._episode_ended = False\n",
    "        \n",
    "        return time_step.restart(self.getObservation())\n",
    "\n",
    "    def _step(self, action):\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        if self.time < self.nStepSimulation:\n",
    "            \n",
    "            self._state = (1-1/self.T) * self._state + self.K/self.T * action\n",
    "            \n",
    "            self.time += 1\n",
    "            return time_step.transition(self.getObservation(), reward = self.getReward(), discount = self.discount)\n",
    "        else:\n",
    "            self._episode_ended = True\n",
    "            return time_step.termination(self.getObservation(), reward = self.getReward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aSimpleUnitTest():\n",
    "    env = MyEnv()\n",
    "    assert isinstance(env, py_environment.PyEnvironment)\n",
    "    utils.validate_py_environment(env, episodes=5)\n",
    "\n",
    "def anotherSimpleUnitTest():\n",
    "    env = MyEnv()\n",
    "    assert isinstance(env, py_environment.PyEnvironment)\n",
    "\n",
    "    u = np.array(np.random.randn(), np.float32) # (,)\n",
    "    \n",
    "    time_step = env.reset()    \n",
    "    rewardAvg = time_step.reward    \n",
    "    while not time_step.is_last():\n",
    "        time_step = env.step(u)\n",
    "        rewardAvg = (1-1/10) * rewardAvg + 1/10 * time_step.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSimpleUnitTest()\n",
    "anotherSimpleUnitTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Represent P-controllers by deterministic policy networks or stochastic ones\n",
    "\n",
    "MyActionNetDeterminisitc and MyActionNetDistiributional are implementations of P-controller with saturated/bounded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.policies import actor_policy\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDeterministic(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(action_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        # action_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)\n",
    "\n",
    "        return _actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActionNetDistributional(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super().__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "        self._layer = tf.keras.layers.Dense(action_spec.shape.num_elements(), activation=tf.nn.tanh)\n",
    "        self._log_action_std = tf.Variable(tf.zeros(shape=())) # (,)\n",
    "        # action_spec\n",
    "        # BoundedTensorSpec(shape=(3,), dtype=tf.float32, name=None, minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        _observations = tf.cast(observations, dtype=tf.float32) # (nPv,)\n",
    "        _actions = self._layer(_observations) # (nMv,)\n",
    "        _actions = tf.reshape(_actions, [-1] + self._output_tensor_spec.shape.as_list()) # (1, nMv)        \n",
    "        _action_std = tf.ones_like(_actions) * tf.math.exp(self._log_action_std) # (1, nMv)\n",
    "\n",
    "        return tfp.distributions.MultivariateNormalDiag(_actions, _action_std), network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnInstanceOfDeterministicPolicy(input_tensor_spec, action_spec):\n",
    "    time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "    return actor_policy.ActorPolicy(\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec    = action_spec,\n",
    "        actor_network  = MyActionNetDeterministic(input_tensor_spec, action_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnInstanceOfDistributionalPolicy(input_tensor_spec, action_spec):\n",
    "    time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "    return actor_policy.ActorPolicy(\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec    = action_spec,\n",
    "        actor_network  = MyActionNetDistributional(input_tensor_spec, action_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPv = 1\n",
    "nMv = 1\n",
    "batch_size = 2**5\n",
    "\n",
    "input_tensor_spec = tensor_spec.TensorSpec((nPv,)\n",
    "                                           , tf.float32)\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec((nMv,),\n",
    "                                            tf.float32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "\n",
    "for my_actor_policy in (createAnInstanceOfDeterministicPolicy(input_tensor_spec, action_spec)\n",
    "                        ,createAnInstanceOfDistributionalPolicy(input_tensor_spec, action_spec)):\n",
    "\n",
    "    observations = tf.random.normal(shape=(batch_size, nPv))\n",
    "\n",
    "    time_step = ts.restart(observations, batch_size) # time_step.is_first = True\n",
    "\n",
    "    action_step = my_actor_policy.action(time_step) # action_step.action: (*, nMv)\n",
    "\n",
    "    distribution_step = my_actor_policy.distribution(time_step)\n",
    "    \n",
    "    assert isinstance(distribution_step.action, tfp.distributions.Distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
